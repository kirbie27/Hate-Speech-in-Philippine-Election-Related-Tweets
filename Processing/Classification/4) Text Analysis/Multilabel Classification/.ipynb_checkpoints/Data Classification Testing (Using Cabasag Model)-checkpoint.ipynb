{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d198f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kirby\n",
      "[nltk_data]     Wenceslao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import validators\n",
    "import emoji\n",
    "import unidecode\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "import torch\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30324c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare Stop Words\n",
    "\n",
    "filipino_stopwords = set(\n",
    "    \"\"\"\n",
    "akin\n",
    "aking\n",
    "ako\n",
    "alin\n",
    "am\n",
    "amin\n",
    "aming\n",
    "ang\n",
    "ano\n",
    "anumang\n",
    "apat\n",
    "at\n",
    "atin\n",
    "ating\n",
    "ay\n",
    "bababa\n",
    "bago\n",
    "bakit\n",
    "bawat\n",
    "bilang\n",
    "dahil\n",
    "dalawa\n",
    "dapat\n",
    "din\n",
    "dito\n",
    "doon\n",
    "gagawin\n",
    "gayunman\n",
    "ginagawa\n",
    "ginawa\n",
    "ginawang\n",
    "gumawa\n",
    "gusto\n",
    "habang\n",
    "hanggang\n",
    "hindi\n",
    "huwag\n",
    "iba\n",
    "ibaba\n",
    "ibabaw\n",
    "ibig\n",
    "ikaw\n",
    "ilagay\n",
    "ilalim\n",
    "ilan\n",
    "inyong\n",
    "isa\n",
    "isang\n",
    "itaas\n",
    "ito\n",
    "iyo\n",
    "iyon\n",
    "iyong\n",
    "ka\n",
    "kahit\n",
    "kailangan\n",
    "kailanman\n",
    "kami\n",
    "kanila\n",
    "kanilang\n",
    "kanino\n",
    "kanya\n",
    "kanyang\n",
    "kapag\n",
    "kapwa\n",
    "karamihan\n",
    "katiyakan\n",
    "katulad\n",
    "kaya\n",
    "kaysa\n",
    "ko\n",
    "kong\n",
    "kulang\n",
    "kumuha\n",
    "kung\n",
    "laban\n",
    "lahat\n",
    "lamang\n",
    "likod\n",
    "lima\n",
    "maaari\n",
    "maaaring\n",
    "maging\n",
    "mahusay\n",
    "makita\n",
    "marami\n",
    "marapat\n",
    "masyado\n",
    "may\n",
    "mayroon\n",
    "mga\n",
    "minsan\n",
    "mismo\n",
    "mula\n",
    "muli\n",
    "na\n",
    "nabanggit\n",
    "naging\n",
    "nagkaroon\n",
    "nais\n",
    "nakita\n",
    "namin\n",
    "napaka\n",
    "narito\n",
    "nasaan\n",
    "ng\n",
    "ngayon\n",
    "ni\n",
    "nila\n",
    "nilang\n",
    "nito\n",
    "niya\n",
    "niyang\n",
    "noon\n",
    "o\n",
    "pa\n",
    "paano\n",
    "pababa\n",
    "paggawa\n",
    "pagitan\n",
    "pagkakaroon\n",
    "pagkatapos\n",
    "palabas\n",
    "pamamagitan\n",
    "panahon\n",
    "pangalawa\n",
    "para\n",
    "paraan\n",
    "pareho\n",
    "pataas\n",
    "pero\n",
    "pumunta\n",
    "pumupunta\n",
    "sa\n",
    "saan\n",
    "sabi\n",
    "sabihin\n",
    "sarili\n",
    "sila\n",
    "sino\n",
    "siya\n",
    "tatlo\n",
    "tayo\n",
    "tulad\n",
    "tungkol\n",
    "una\n",
    "walang\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "search = \"leni robredo bongbong marcos isko moreno domagoso manny pacman pacquiao ping lacson ernie abella leody de guzman norberto gonzales jose montemayor jr faisal mangondato\"\n",
    "candidatelist = search.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453cf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Models and files\n",
    "    \n",
    "with open('x_train.pkl', 'rb') as file:\n",
    "    x_train = pickle.load(file)\n",
    "    \n",
    "with open('x_test.pkl', 'rb') as file:\n",
    "    x_test = pickle.load(file)\n",
    "\n",
    "with open('y_train.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "    \n",
    "with open('y_test.pkl', 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "    \n",
    "with open('tweet_collection.pkl', 'rb') as file:\n",
    "    tweet_collection = pickle.load(file)   \n",
    "    \n",
    "#Convert Labels from Strings to categorical Integers {Non-Hate = 1, Hate = 0}\n",
    "\n",
    "df_ytrain = pd.DataFrame(y_train, columns = ['Label'])\n",
    "df_ytest = pd.DataFrame(y_test, columns = ['Label'])\n",
    "\n",
    "mapping = {'Non-hate': 0, 'Hate': 1}\n",
    "df_ytrain = df_ytrain.replace({'Label': mapping})\n",
    "df_ytest = df_ytest.replace({'Label': mapping})\n",
    "\n",
    "\n",
    "train_y = df_ytrain['Label'].tolist()\n",
    "test_y = df_ytest['Label'].tolist()\n",
    "\n",
    "np_train_y = np.asarray(train_y)\n",
    "np_test_y = np.asarray(test_y)\n",
    "    \n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  \n",
    "\n",
    "#classifier building/ fitting of training dataset to tfidf\n",
    "fitted_training_x = tfidf.fit_transform(x_train)\n",
    "\n",
    "#transform based on top 40 percent features\n",
    "selector = SelectPercentile(f_classif, percentile = 40)\n",
    "selector.fit(fitted_training_x, train_y)\n",
    "\n",
    "#load model\n",
    "cabasag_model = load_model(\"cabasag_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process tweet input\n",
    "def pre_process_tweet(tweet_input):\n",
    "    #Step 1 - Extract Tweet from input\n",
    "    #Tweet = tweet_input\n",
    "    tweet = tweet_input.strip().replace(\"\\n\",\" \")\n",
    "    print(\"\\nExtracted tweet: \\n\",tweet, sep = \"\")\n",
    "    #Step 2 - Data Deidentification\n",
    "    output = \"\"\n",
    "    sentence = tweet.split(\" \")\n",
    "    for part in sentence:\n",
    "        if not re.match(r\"(^|[^@\\w])@(\\w{1,15})\\b\", part):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "\n",
    "    tweets_de_identified = output\n",
    "    print(\"\\nDe-identified tweet: \\n\",tweets_de_identified, sep = \"\")\n",
    "    #Step 3 - URL Removal\n",
    "    output = \"\"\n",
    "    sentence = tweets_de_identified.split(\" \")\n",
    "    for part in sentence:\n",
    "        valid = validators.url(part)\n",
    "\n",
    "        if (not valid == True):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "                \n",
    "    tweets_url_removed = output\n",
    "    print(\"\\nURL Removed tweet: \\n\",tweets_url_removed, sep = \"\")\n",
    "    #Step 4 - Special Character Processing\n",
    "    \n",
    "    emoji_removed = emoji.replace_emoji(tweets_url_removed, replace=\"[emoji]\")\n",
    "    output = \"\"\n",
    "    sentence = emoji_removed.split(\" \")\n",
    "\n",
    "    for part in sentence:\n",
    "        if not (re.match(r\"^[_\\W]+$\", part) or \"[emoji]\" in part):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "    \n",
    "    tweets_specialcharacters_removed = output\n",
    "    print(\"\\nSpecial Characters removed tweet: \\n\",tweets_specialcharacters_removed, sep = \"\")\n",
    "    #Step 5 - Normalization, lowercase>removediacritics>remove numerics and symbols>stopwords\n",
    "    \n",
    "    #lowercase the text\n",
    "    lowercased_input = tweets_specialcharacters_removed.lower()\n",
    "\n",
    "    #remove diacritics\n",
    "    diacritics_removed = unidecode.unidecode(lowercased_input)\n",
    "\n",
    "    output = \"\"\n",
    "    sentence = diacritics_removed.split(\" \")\n",
    "\n",
    "    for part in sentence:\n",
    "        part = re.sub(\"[^A-Za-z ]+$\", \"\", part)\n",
    "        part = re.sub(\"^[^A-Za-z #]+\", \"\", part)\n",
    "        if not (len(part) <= 1 or re.match(r\"[^a-zA-Z]\", part) or part in english_stopwords or part in filipino_stopwords or any(x in part for x in candidatelist)):     \n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"  \n",
    "                \n",
    "    tweets_normalized = output\n",
    "    print(\"\\nNormalized tweet: \\n\",tweets_normalized, sep = \"\")\n",
    "    \n",
    "    #Step 6 - Hashtag Processing, removing the hashtags from the tweet\n",
    "    output = \"\"\n",
    "    sentence = tweets_normalized.split(\" \")\n",
    "\n",
    "    for part in sentence:\n",
    "        if not re.match(r\"#(\\w+)\", part):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "                \n",
    "    tweets_hashtags_removed = output  \n",
    "    print(\"\\nHashtags removed tweet: \\n\",tweets_hashtags_removed, sep = \"\")\n",
    "    #Step 7 - Tokenization\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    \n",
    "    output = tokenizer.tokenize(tweets_hashtags_removed)\n",
    "    \n",
    "    tweets_tokenized = output\n",
    "    print(\"\\nTokenized tweet: \\n\",tweets_tokenized, sep = \"\")\n",
    "    print(\"\\n\",sep = \"\")\n",
    "    return tweets_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35150d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(tokenized_tweet):\n",
    "    transformed = tfidf.transform([tokenized_tweet])\n",
    "    return selector.transform(transformed).toarray()\n",
    "\n",
    "def cabasag_predict(tweet, label):\n",
    "    start_time = time.time()\n",
    "    print(\"Input tweet: \\n\", tweet, sep = \"\")\n",
    "    tokenized_tweet = pre_process_tweet(tweet)\n",
    "    vectorized_tweet = tfidf_vectorizer(tokenized_tweet)\n",
    "    \n",
    "    prediction_noround = cabasag_model.predict(vectorized_tweet)\n",
    "    run_time = time.time() - start_time\n",
    "    prediction = np.argmax(prediction_noround)\n",
    "    #model.predict\n",
    "    \n",
    "    classification = \"Positive\"\n",
    "\n",
    "    if(prediction == 1):\n",
    "        classification = \"Negative\"\n",
    "        \n",
    "    if(prediction == 2):\n",
    "        classification = \"Neutral\"\n",
    "        \n",
    "    print(\"\\nUsing Multilabel Classification fastTextCNN Model\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\nInput text:\\n\"\n",
    "      , tweet, \"\\n\\nTrue Label:\\n- \",label,\"\\n\\nModel Classification:\\n- \", classification,\n",
    "      \"\\n\\nClassification Output:\\n- \",prediction_noround,\"\\n- \", prediction, \"\\n\\nRuntime: \\n- \",\n",
    "      run_time, \" seconds.\", sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81d716ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tweet: \n",
      "\n",
      "@edupunay @PhilippineStar Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. ðŸ˜› Solid BBM kami! Solid UNITEAM â¤ï¸ðŸ’š\n",
      "#BBMIsMyPresident2022\n",
      "#BBMSARA2022\n",
      "\n",
      "\n",
      "Extracted tweet: \n",
      "@edupunay @PhilippineStar Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. ðŸ˜› Solid BBM kami! Solid UNITEAM â¤ï¸ðŸ’š #BBMIsMyPresident2022 #BBMSARA2022\n",
      "\n",
      "De-identified tweet: \n",
      "Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. ðŸ˜› Solid BBM kami! Solid UNITEAM â¤ï¸ðŸ’š #BBMIsMyPresident2022 #BBMSARA2022\n",
      "\n",
      "URL Removed tweet: \n",
      "Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. ðŸ˜› Solid BBM kami! Solid UNITEAM â¤ï¸ðŸ’š #BBMIsMyPresident2022 #BBMSARA2022\n",
      "\n",
      "Special Characters removed tweet: \n",
      "Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. Solid BBM kami! Solid UNITEAM #BBMIsMyPresident2022 #BBMSARA2022\n",
      "\n",
      "Normalized tweet: \n",
      "hahaha nenega talaga kakampinks nalang tanggapin panalo si number balota nag papabrainwash kay lugaw solid bbm solid uniteam\n",
      "\n",
      "Hashtags removed tweet: \n",
      "hahaha nenega talaga kakampinks nalang tanggapin panalo si number balota nag papabrainwash kay lugaw solid bbm solid uniteam\n",
      "\n",
      "Tokenized tweet: \n",
      "['hahaha', 'nenega', 'talaga', 'kakampinks', 'nalang', 'tanggapin', 'panalo', 'si', 'number', 'balota', 'nag', 'papabrainwash', 'kay', 'lugaw', 'solid', 'bbm', 'solid', 'uniteam']\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "Using Multilabel Classification fastTextCNN Model\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Input text:\n",
      "\n",
      "@edupunay @PhilippineStar Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. ðŸ˜› Solid BBM kami! Solid UNITEAM â¤ï¸ðŸ’š\n",
      "#BBMIsMyPresident2022\n",
      "#BBMSARA2022\n",
      "\n",
      "\n",
      "True Label:\n",
      "- Negative\n",
      "\n",
      "Model Classification:\n",
      "- Negative\n",
      "\n",
      "Classification Output:\n",
      "- [[2.1747590e-07 9.9998105e-01 1.8673116e-05]]\n",
      "- 1\n",
      "\n",
      "Runtime: \n",
      "- 0.04886984825134277 seconds.\n"
     ]
    }
   ],
   "source": [
    "input_tweet = '''\n",
    "@edupunay @PhilippineStar Hahaha ang nenega talaga ng kakampinks! Hindi nalang tanggapin na panalo na si Bongbong Marcos number 7 sa balota!! Hindi kami nag papabrainwash kay Leni Lugaw. ðŸ˜› Solid BBM kami! Solid UNITEAM â¤ï¸ðŸ’š\n",
    "#BBMIsMyPresident2022\n",
    "#BBMSARA2022\n",
    "'''\n",
    "input_label = \"Negative\"\n",
    "\n",
    "cabasag_predict(input_tweet, input_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0733ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4f509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
